#+TITLE: Red Wine Quality Classifier
#+AUTHOR: João Lucas Correia Barbosa de Farias
#+AUTHOR: Júlio Freire
#+EMAIL: joao.farias.080@ufrn.edu.br

* Deployment of a full neural network pipeline

This project consists of recognition of german traffic signs. A full pipeline was created to handle processing of the features and the training of the model, which was performed using a LeNet-5 Neural Network. Then, the model and target encoder were exported to [[https://wandb.ai/site][Weights & Biases]] (W&B) where they were later retrieved for the deployment of the model. The deployment was possible due to [[https://github.com/][GitHub]] CI/CD integration with [[https://www.heroku.com/][Heroku]]. The final product is available for testing at [[https://traffic-sign-reco.herokuapp.com/][Traffic Sign Recognition App]] where anyone can input the values for the wine features and get back a result for its quality: either 'good' or 'bad'. The dataset was taken from [[https://www.kaggle.com/datasets/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign][Kaggle]].

* About the Authors
The authors of this project are [[https://github.com/jotafarias13][João Lucas Farias]] and [[https://github.com/juliofreire][Júlio Freire]]. The project was created as part of a Graduate Level course on Machine Learning at the Graduate Program in Electrical and Computer Engineering at the Federal University of Rio Grande do Norte (UFRN), Brasil. The course is taught by [[https://github.com/ivanovitchm][Professor Ivanovitch Silva]]. The goal of the project was to teach the students how to create a Pipeline for Machine Learning algorithms (along with many good practices in ML programming), train the model and deploy the exported model for production. This way, anyone can test and verify the performance of the model. By using [[https://wandb.ai/site][Weights & Biases]], [[https://github.com/][GitHub]] and [[https://www.heroku.com/][Heroku]], we were able to train the model and deploy it.

* From data fetch to full pipeline
In order to perform the proposed task in a clear and organized way, we split each stage of implementation in a single file. The files can be found at the [[file:source/pipeline/][pipeline directory]]. From data fetch to a full pipeline passing through preprocessing, data check and segretation, we implemented the code necessary to train and test the model. All coding was done on [[https://colab.research.google.com/][Google Colab]] so as to encourage and facilitate colaboration.

** Data Fetch
In this [[file:source/pipeline/1-fetch_data.ipynb][first file]], we mounted our Google Drive to Colab to make it possible to access the files inside our Drive. This way we were able to upload the data to our [[https://wandb.ai/ppgeec-ml-jj][colaborative team]] on W&B. In this link of our colaborative team, you can find the project [[https://wandb.ai/ppgeec-ml-jj/traffic_sign_recognition/overview][traffic_sign_recognition]] and can see all the runs we created with W&B when working on this project. Also, all the artifacts that were exported to W&B can be found in the [[https://wandb.ai/ppgeec-ml-jj/traffic_sign_recognition/artifacts][Artifacts page]].

Before uploading to W&B, we decided to use only a portion of the whole dataset. The entire dataset has over 50k images. So, in order to facilitate hyperparameter tuning on the free version of Google Colab, we selected 30% of those images at random. When downloaded, the dataset was already split between Train and Test sets with a 80/20 percent ratio. The [[https://wandb.ai/ppgeec-ml-jj/traffic_sign_recognition/artifacts/raw_data/raw_data_train.h5][Train]] and [[https://wandb.ai/ppgeec-ml-jj/traffic_sign_recognition/artifacts/raw_data/raw_data_test.h5/v2][Test]] images were exported to W&B as HDF5 files. Also, the respective labels for [[https://wandb.ai/ppgeec-ml-jj/traffic_sign_recognition/artifacts/raw_data/raw_data_train_labels.csv/v0][train]] and [[https://wandb.ai/ppgeec-ml-jj/traffic_sign_recognition/artifacts/raw_data/raw_data_test_labels.csv/v1][test]] sets were exported to W&B as csv files.

** Exploratory Data Analysis (EDA)
In this [[file:source/pipeline/2-eda.ipynb][second file]], we performed the exploratory data analysis, which consists in going over the whole dataset, looking for patterns, correlation between features and possible outliers. This analysis is based on statistical tools that helps us understand and visualize our data in a proper manner. Here, we found there were no duplicate images in our dataset. Also, we noticed we had an imbalanced dataset with some signs having much more images than others.

Since there were no anomalies with the dataset, there was no need for a preprocessing stage before the pipeline.

** Data Check
In this [[file:source/pipeline/3-data_check.ipynb][third]] step, we performed some tests on the preprocessed dataset to check if all the columns were in order and according to what we expected. Basically, we created some test functions to check datatypes and ranges of the features and execute the tests with pytest.

** Data Segretation
In this [[file:source/pipeline/4-data_segregation.ipynb][fourth file]], we did not have to perform any segregation because, as mentioned, the dataset was segregated previous to the download. This way we only download the raw data from W&B and exported as 'segregated_data' for consistency. The segretated [[https://wandb.ai/ppgeec-ml-jj/traffic_sign_recognition/artifacts/segregated_data/train.h5/v0][Train]] data and respective [[https://wandb.ai/ppgeec-ml-jj/traffic_sign_recognition/artifacts/segregated_data/train_labels.csv/v0][labels]] as well as [[https://wandb.ai/ppgeec-ml-jj/traffic_sign_recognition/artifacts/segregated_data/test.h5/v2][Test]] data and respective [[https://wandb.ai/ppgeec-ml-jj/traffic_sign_recognition/artifacts/segregated_data/test_labels.csv/v1][labels]] can be found in the Artifacts page of our project.

** Train
In this [[file:source/pipeline/5-train.ipynb][fifth]] and longest step, we collected the segregated train data from W&B and split it into two new sets: train and validation sets. The validation set is 10% of the original train set. The train set is used to train the model while the validation set is used to analyze the metrics and tune the hyperparameters. So, we first train the model with the train set, then validate the trained model with the validation set.

But, before training, we had to create our pipeline. Since we only had numerical features, we did not need to process categorical features. Since the images all have different sizes, that is, number of pixels, we resized the images to have 30x30 pixels. After converting the images to numpy arrays, we normalized the values. These values are from 0 to 255, representing each channels of a RGB image. So, we divided the whole train set of images by 255.

After that, we moved on to the training step. We used a LeNet-5 Neural Network as the classifier and analyzed the Accuracy of the model over the validation set. We performed Hyperparameter Tuning with the help of W&B sweeps. We configured the sweep to test some configurations for our training summing up to over 50k combinations. Of those, we selected 100 to try out and look for the best one.

Overall, we tried some learning, generalization and batch-normalization tools. For learning, we considered different loss functions, learning rates, using relu activation function to fix vanishing gradient and using gradient clipping to fix exploding gradient. Also, for generalization, we considered the addition of Dropout layers to avoid overfitting and halt training early with Early Stopping callbacks. Finally, we tested whether the use of batch-normalization would benefit the classification process.

After running these different configurations, the W&B [[https://wandb.ai/ppgeec-ml-jj/traffic_sign_recognition/sweeps/ftpjniuf/overview][sweep]] showed us the [[https://wandb.ai/ppgeec-ml-jj/traffic_sign_recognition/runs/9a76ir5b/overview][best]] result (the one with the highest accuracy). This configuration model was then used as our best model. Finally, the pipeline and best model were exported to W&B and can be seen in the Artifacts page ([[https://wandb.ai/ppgeec-ml-jj/traffic_sign_recognition/artifacts/inference_artifact/pipeline/v0][pipeline]] and [[https://wandb.ai/ppgeec-ml-jj/traffic_sign_recognition/artifacts/inference_artifact/model.h5/v1][model]]).

** Test
In this [[file:source/pipeline/6-test.ipynb][sixth and final]] step, we tested our model against the test set. Our final metrics were as shown below and can be seen in this W&B [[https://wandb.ai/ppgeec-ml-jj/traffic_sign_recognition/runs/10d8vuzy/overview][run]].

26-07-2022 00:05:09 Test Accuracy: 0.9097

26-07-2022 00:05:09 Test Precision: 0.9113

26-07-2022 00:05:09 Test Recall: 0.9097

26-07-2022 00:05:09 Test F1: 0.9079

* References
This project was based on our previous project on a  [[https://github.com/juliofreire/wine-quality-ml][Decision Tree classifier model]] used to predict red wine quality by analyzing chemical proprieties of wines. Also, we based ourselves on the LeNet-5 Neural Network implementation as perfomed by [[https://github.com/ivanovitchm/deeplearning][Ivanovitch Silva]].

